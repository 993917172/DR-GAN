{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple_DR_GAN\n",
    "\n",
    "ジュネレータに同じ人の複数の画像（異なるポーズ，シーンetc)を入れ，出力されたそれぞれの特徴量を\n",
    "重み付けして足し合わせた特徴量を元に画像を生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator の定義\n",
    "\n",
    "single-image_DR_GAN と同じ\n",
    "> - 論文で用いられている  TensorFlow のConv オプション padding=\"SAME\"と同じ挙動を再現するために padding layer を間に追加\n",
    "- 入力は バッチ数(B)ｘ96x96x3\n",
    "- 個人の識別(Nd+1) と　姿勢の推定(Np)を同時に行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, Nd, Np):\n",
    "        super(Discriminator, self).__init__()\n",
    "        convLayers = [\n",
    "            nn.Conv2d(3, 32, 3, 1, 1, bias=False), # Bx3x96x96 -> Bx32x96x96\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False), # Bx32x96x96 -> Bx64x96x96\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # Bx64x96x96 -> Bx64x97x97\n",
    "            nn.Conv2d(64, 64, 3, 2, 0, bias=False), # Bx64x97x97 -> Bx64x48x48\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=False), # Bx64x48x48 -> Bx64x48x48\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), # Bx64x48x48 -> Bx128x48x48\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # Bx128x48x48 -> Bx128x49x49\n",
    "            nn.Conv2d(128, 128, 3, 2, 0, bias=False), #  Bx128x49x49 -> Bx128x24x24\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(128, 96, 3, 1, 1, bias=False), #  Bx128x24x24 -> Bx96x24x24\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(96, 192, 3, 1, 1, bias=False), #  Bx96x24x24 -> Bx192x24x24\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # Bx192x24x24 -> Bx192x25x25\n",
    "            nn.Conv2d(192, 192, 3, 2, 0, bias=False), # Bx192x25x25 -> Bx192x12x12\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(192, 128, 3, 1, 1, bias=False), # Bx192x12x12 -> Bx128x12x12\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1, bias=False), # Bx128x12x12 -> Bx256x12x12\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # Bx256x12x12 -> Bx256x13x13\n",
    "            nn.Conv2d(256, 256, 3, 2, 0, bias=False),  # Bx256x13x13 -> Bx256x6x6\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 160, 3, 1, 1, bias=False), # Bx256x6x6 -> Bx160x6x6\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(160, 320, 3, 1, 1, bias=False), # Bx160x6x6 -> Bx320x6x6\n",
    "            nn.BatchNorm2d(320),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(6, stride=1), #  Bx320x6x6 -> Bx320x1x1\n",
    "        ]\n",
    "        \n",
    "        self.convLayers = nn.Sequential(*convLayers)\n",
    "        self.fc = nn.Linear(320, Nd+1+Np)\n",
    "        \n",
    "        # 重みは全て N(0, 0.02) で初期化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                \n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # 畳み込み -> 平均プーリングの結果 B x 320 x 1 x 1の出力を得る\n",
    "        x = self.convLayers(input)\n",
    "        \n",
    "        # バッチ数次元を消さないように１次元の次元を削除　\n",
    "        x = x.squeeze(2)\n",
    "        x = x.squeeze(2)\n",
    "        \n",
    "        # 全結合 \n",
    "        x = self.fc(x) # Bx320 -> B x (Nd+1+Np)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator の定義\n",
    "\n",
    "- G_enc は 同一人物に n 枚の画像があるとして nB x 1 x 96 x 96 -> B x n x 321 -> B x 320 と特徴量をencode\n",
    "- G_dec は single-image DR_GANと同じ\n",
    "> single-image の時\n",
    "    - G_enc は Discriminator と最後の全結合層が無い以外同じ構造\n",
    "    - G_dec のアップサンプリング時は，ダウンサンプリング時に Zeropadding を行なったことの逆で，ConvTranspose2d 後に Crop（negative padding?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## nn.Module を継承しても， super でコンストラクタを呼び出さないと メンバ変数 self._modues が\n",
    "## 定義されずに後の重み初期化の際にエラーを出す\n",
    "## sef._modules はモジュールが格納するモジュール名を格納しておくリスト\n",
    "\n",
    "class Crop(nn.Module):\n",
    "    def __init__(self, crop_list):\n",
    "        super().__init__()\n",
    "        \n",
    "        # crop_lsit = [crop_top, crop_bottom, crop_left, crop_right]\n",
    "        self.crop_list = crop_list\n",
    "            \n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.size()\n",
    "        x = x[:,:, self.crop_list[0] : H - self.crop_list[1] , self.crop_list[2] : W - self.crop_list[3]]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重みの出力の部分にだけシグモイド関数をかけ， その重みを用いて n枚の画像の出力結果を足し合わせる\n",
    "# 入力： nBx321x1x1　-> 出力: B x 320x1x1\n",
    "\n",
    "class WSum_feature(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 一人にあたり何枚の画像を渡しているのか指示\n",
    "        self.n = n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CNN出力結果を特徴量と重みに分けた後に， 各人 n 枚毎のデータへ分割\n",
    "        features = x[:,:-1].split(self.n,0)\n",
    "        weights = x[:,-1].unsqueeze(1).sigmoid().split(self.n,0)\n",
    "        \n",
    "        features_summed = []\n",
    "        \n",
    "        # nBx320x1x1 -> Bx320x1x1\n",
    "        for (feature_each, weight_each)  in zip(features, weights):\n",
    "            feature_weighted = feature_each*weight_each\n",
    "            feature_summed = feature_weighted.sum(0, keepdim=True) / weight_each.sum(0)\n",
    "            features_summed.append(feature_summed)\n",
    "        \n",
    "        features_summed = torch.cat(features_summed)\n",
    "        \n",
    "        return features_summed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "B = 50\n",
    "w = np.arange(300).reshape(300,1)\n",
    "tmp = np.ones((n*B, 321))\n",
    "x = Variable(torch.FloatTensor(w*tmp), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 321])\n",
      "torch.Size([300, 1])\n"
     ]
    }
   ],
   "source": [
    "weight = x[:,-1].unsqueeze(1)#.sigmoid()\n",
    "features = x*weight\n",
    "features = features[:,:-1].split(n, 0)\n",
    "features = torch.cat(features,1)\n",
    "features = features.sum(0, keepdim=True)\n",
    "features = features.view(50,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = features.sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 5.5000e+01  5.5000e+01  5.5000e+01  ...   5.5000e+01  5.5000e+01  5.5000e+01\n",
       " 4.5100e+02  4.5100e+02  4.5100e+02  ...   4.5100e+02  4.5100e+02  4.5100e+02\n",
       " 1.2790e+03  1.2790e+03  1.2790e+03  ...   1.2790e+03  1.2790e+03  1.2790e+03\n",
       "                ...                   ⋱                   ...                \n",
       " 4.8566e+05  4.8566e+05  4.8566e+05  ...   4.8566e+05  4.8566e+05  4.8566e+05\n",
       " 5.0636e+05  5.0636e+05  5.0636e+05  ...   5.0636e+05  5.0636e+05  5.0636e+05\n",
       " 5.2749e+05  5.2749e+05  5.2749e+05  ...   5.2749e+05  5.2749e+05  5.2749e+05\n",
       "[torch.FloatTensor of size 50x320]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.view(50,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "def WSum_feature(x, n):\n",
    "    \"\"\"\n",
    "    重みの出力の部分にだけシグモイド関数をかけ， その重みを用いて n枚の画像の出力結果を足し合わせる\n",
    "    入力： nBx321x1x1　-> 出力: B x 320x1x1\n",
    "\n",
    "    n : 一人にあたり何枚の画像をデータとして渡しているのか\n",
    "\n",
    "    \"\"\"\n",
    "    # nBx320x1x1 -> Bx320x1x1\n",
    "    weight = x[:,-1].unsqueeze(1).sigmoid()\n",
    "    features = x*weight\n",
    "    features = features[:,:-1].split(n, 0)\n",
    "    features = torch.cat(features,1)\n",
    "    features_summed = features.sum(0, keepdim=True)\n",
    "    features_summed = features_summed.view(50,-1)\n",
    "    \n",
    "    return features_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 5.5000e+01  5.5000e+01  5.5000e+01  ...   5.5000e+01  5.5000e+01  5.5000e+01\n",
       " 4.5100e+02  4.5100e+02  4.5100e+02  ...   4.5100e+02  4.5100e+02  4.5100e+02\n",
       " 1.2790e+03  1.2790e+03  1.2790e+03  ...   1.2790e+03  1.2790e+03  1.2790e+03\n",
       "                ...                   ⋱                   ...                \n",
       " 4.8566e+05  4.8566e+05  4.8566e+05  ...   4.8566e+05  4.8566e+05  4.8566e+05\n",
       " 5.0636e+05  5.0636e+05  5.0636e+05  ...   5.0636e+05  5.0636e+05  5.0636e+05\n",
       " 5.2749e+05  5.2749e+05  5.2749e+05  ...   5.2749e+05  5.2749e+05  5.2749e+05\n",
       "[torch.FloatTensor of size 50x320]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WSum_feature(x,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, Np, Nz, n):\n",
    "        super(Generator, self).__init__()\n",
    "        G_enc_convLayers = [\n",
    "            nn.Conv2d(3, 32, 3, 1, 1, bias=False), # nBx3x96x96 -> nBx32x96x96\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False), # nBx32x96x96 -> nBx64x96x96\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # nBx64x96x96 -> nBx64x97x97\n",
    "            nn.Conv2d(64, 64, 3, 2, 0, bias=False), # nBx64x97x97 -> nBx64x48x48\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=False), # nBx64x48x48 -> nBx64x48x48\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), # nBx64x48x48 -> nBx128x48x48\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # nBx128x48x48 -> nBx128x49x49\n",
    "            nn.Conv2d(128, 128, 3, 2, 0, bias=False), #  nBx128x49x49 -> nBx128x24x24\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(128, 96, 3, 1, 1, bias=False), #  nBx128x24x24 -> nBx96x24x24\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(96, 192, 3, 1, 1, bias=False), #  nBx96x24x24 -> nBx192x24x24\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # nBx192x24x24 -> nBx192x25x25\n",
    "            nn.Conv2d(192, 192, 3, 2, 0, bias=False), # nBx192x25x25 -> nBx192x12x12\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(192, 128, 3, 1, 1, bias=False), # nBx192x12x12 -> nBx128x12x12\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1, bias=False), # nBx128x12x12 -> nBx256x12x12\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),                      # nBx256x12x12 -> nBx256x13x13\n",
    "            nn.Conv2d(256, 256, 3, 2, 0, bias=False),  # nBx256x13x13 -> nBx256x6x6\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 160, 3, 1, 1, bias=False), # nBx256x6x6 -> nBx160x6x6\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            # 同一人物の画像の特徴量を足し合わせる際の重みを示す値 w を１次元分チャネルに追加\n",
    "            nn.Conv2d(160, 321, 3, 1, 1, bias=False), # nBx160x6x6 -> nBx321x6x6\n",
    "            nn.BatchNorm2d(321),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(6, stride=1), #  nBx321x6x6 -> nBx321x1x1\n",
    "            \n",
    "            # 同一人物の画像の特徴量を重みを用いて足し合わせる\n",
    "            WSum_feature(n), # nBx321x1x1 -> Bx320x1x1\n",
    "        ]\n",
    "        self.G_enc_convLayers = nn.Sequential(*G_enc_convLayers)\n",
    "        \n",
    "        G_dec_convLayers = [\n",
    "            nn.ConvTranspose2d(320,160, 3,1,1, bias=False), # Bx320x6x6 -> Bx160x6x6\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(160, 256, 3,1,1, bias=False), # Bx160x6x6 -> Bx256x6x6\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(256, 256, 3,2,0, bias=False), # Bx256x6x6 -> Bx256x13x13\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            Crop([0, 1, 0, 1]),\n",
    "            nn.ConvTranspose2d(256, 128, 3,1,1, bias=False), # Bx256x12x12 -> Bx128x12x12  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(128, 192,  3,1,1, bias=False), # Bx128x12x12 -> Bx192x12x12            \n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(192, 192,  3,2,0, bias=False), # Bx128x12x12 -> Bx192x25x25            \n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(),\n",
    "            Crop([0, 1, 0, 1]),\n",
    "            nn.ConvTranspose2d(192, 96,  3,1,1, bias=False), # Bx192x24x24 -> Bx96x24x24 \n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(96, 128,  3,1,1, bias=False), # Bx96x24x24 -> Bx128x24x24\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(128, 128,  3,2,0, bias=False), # Bx128x24x24 -> Bx128x49x49      \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            Crop([0, 1, 0, 1]),\n",
    "            nn.ConvTranspose2d(128, 64,  3,1,1, bias=False), # Bx128x48x48 -> Bx64x48x48\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(64, 64,  3,1,1, bias=False), # Bx64x48x48 -> Bx64x48x48  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(64, 64,  3,2,0, bias=False), # Bx64x48x48 -> Bx64x97x97  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            Crop([0, 1, 0, 1]),\n",
    "            nn.ConvTranspose2d(64, 32,  3,1,1, bias=False), # Bx64x96x96 -> Bx32x96x96 \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(32, 3,  3,1,1, bias=False), # Bx32x96x96 -> Bx3x96x96 \n",
    "            nn.ELU(),\n",
    "        ]\n",
    "        \n",
    "        self.G_dec_convLayers = nn.Sequential(*G_dec_convLayers)\n",
    "        \n",
    "        self.G_dec_fc = nn.Linear(320+Np+Nz, 320*6*6)\n",
    "        \n",
    "        # 重みは全て N(0, 0.02) で初期化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                \n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                \n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input, pose, noise):\n",
    "        \n",
    "        x = self.G_enc_convLayers(input) # nBx1x96x96 -> Bx320x1x1\n",
    "        \n",
    "        x = x.squeeze(2)\n",
    "        x = x.squeeze(2)\n",
    "        \n",
    "        x = torch.cat([x, pose, noise], 1)  # nBx320 -> nB x (320+Np+Nz)\n",
    "        \n",
    "        x = self.G_dec_fc(x) # B x (320+Np+Nz) -> B x (320x6x6)\n",
    "    \n",
    "        x = x.view(-1, 320, 6, 6) # B x (320x6x6) -> B x 320 x 6 x 6\n",
    "        \n",
    "        x = self.G_dec_convLayers(x) #  B x 320 x 6 x 6 -> Bx1x96x96\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ランダム入力データの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_size = 80\n",
    "imnum_each_person = 5\n",
    "Nd = 200\n",
    "Np = 9\n",
    "\n",
    "images = np.random.randn(data_size, 3, 96,96)\n",
    "# id は0~199 と仮定\n",
    "id_labels = np.random.randint(Nd, size=data_size)\n",
    "pose_labels = np.random.randint(Np, size=data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator の正解率を算出し， 指定した以上の正解率であれば， 十分強いとみなす\n",
    "\n",
    "def Is_D_strong(real_output, syn_output, id_label_tensor, pose_label_tensor, syn_id_label_tensor, Nd, batch_size, thresh=0.9):\n",
    "    # Discriminator の正解率を算出\n",
    "    _, id_real_ans = torch.max(real_output[:, :Nd+1], 1)\n",
    "    _, pose_real_ans = torch.max(real_output[:, Nd+1:], 1)\n",
    "    _, id_syn_ans = torch.max(syn_output[:, :Nd+1], 1)\n",
    "\n",
    "    id_real_precision = (id_real_ans==id_label_tensor).type(torch.FloatTensor).sum() / batch_size\n",
    "    pose_real_precision = (pose_real_ans==pose_label_tensor).type(torch.FloatTensor).sum() / batch_size\n",
    "    id_syn_precision = (id_syn_ans==syn_id_label_tensor).type(torch.FloatTensor).sum() / batch_size\n",
    "    \n",
    "    total_precision = (id_real_precision+pose_real_precision+id_syn_precision)/3\n",
    "    \n",
    "    # Variable(FloatTensor) -> Float へと変換\n",
    "    total_precision = total_precision.data[0]\n",
    "    \n",
    "    if total_precision>=thresh:\n",
    "        flag_D_strong = True\n",
    "    else:\n",
    "        flag_D_strong = False\n",
    "    \n",
    "    return flag_D_strong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10 \n",
    "imnum_each_person = 5 # 一人あたり何枚の画像がデータに含まれているか\n",
    "epoch = 10000\n",
    "image_size = images.shape[0]\n",
    "epoch_time = np.ceil(image_size / batch_size).astype(int)\n",
    "\n",
    "Nd = 200 # number of ID (person)\n",
    "Np = 9 # number of discrite poses\n",
    "Nz = 50 # number of noise dimension\n",
    "\n",
    "lr_Adam = 0.0002 \n",
    "m_Adam = 0.5\n",
    "flag_D_strong  = False\n",
    "\n",
    "D = Discriminator(Nd, Np)\n",
    "G = Generator(Np, Nz, imnum_each_person)\n",
    "optimizer_D = optim.Adam(D.parameters())\n",
    "optimizer_G = optim.Adam(G.parameters())\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    for i in range(epoch_time):\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        batch_image = images[start:end]\n",
    "        batch_id_label = id_labels[start:end]\n",
    "        batch_pose_label = pose_labels[start:end]\n",
    "        minibatch_size = len(batch_image)\n",
    "        \n",
    "        \n",
    "        # 学習の中で使われるVariable変数の定義\n",
    "        # ラベルの定義(CrossEntropy 誤差で用いる際は FloatTensorでは☓)\n",
    "        img_tensor = Variable(torch.FloatTensor(batch_image))\n",
    "        id_label_tensor = Variable(torch.LongTensor(batch_id_label))\n",
    "        id_label_unique_tensor = id_label_tensor[::imnum_each_person]\n",
    "        pose_label_tensor = Variable(torch.LongTensor(batch_pose_label))\n",
    "        \n",
    "        syn_id_labels = Nd*np.ones(minibatch_size//imnum_each_person).astype(int)\n",
    "        syn_id_label_tensor = Variable(torch.LongTensor(syn_id_labels))\n",
    "        \n",
    "        # ノイズと姿勢コードを生成\n",
    "        \n",
    "        # 実際に入力するデータは minibatch_size = n x 人数 で CNNから出力される特徴は   condition_batchsize = 人数\n",
    "        condition_batchsize = int(minibatch_size/imnum_each_person)\n",
    "        \n",
    "        fixed_noise_tensor = Variable(torch.FloatTensor(np.random.uniform(-1,1, (condition_batchsize, Nz))))\n",
    "        pose_code = np.zeros((condition_batchsize, Np))\n",
    "        tmp  = np.random.randint(Np, size=condition_batchsize)\n",
    "        pose_code[:, tmp] = 1\n",
    "        pose_code_label_tensor = Variable(torch.LongTensor(tmp)) # CrossEntropy 誤差に使用\n",
    "        pose_code_tensor = Variable(torch.FloatTensor(pose_code)) # Condition 付に使用\n",
    "        \n",
    "        # Generatorでイメージ生成\n",
    "        generated = G(img_tensor, pose_code_tensor, fixed_noise_tensor)\n",
    "        \n",
    "        # バッチ毎に交互に D と G の学習，　Dが90%以上の精度の場合は 1:4の比率で学習\n",
    "        print(i)\n",
    "        if flag_D_strong:\n",
    "            if i%5 == 0:\n",
    "                # Discriminator の学習\n",
    "                real_output = D(img_tensor)\n",
    "                syn_output = D(generated.detach()) # .detach() をすることでGeneratorのパラメータを更新しない\n",
    "\n",
    "                # id についての出力とラベル, pose についての出力とラベル それぞれの交差エントロピー誤差を計算\n",
    "                \n",
    "                d_loss = loss_criterion(real_output[:, :Nd+1], id_label_tensor) +\\\n",
    "                                        loss_criterion(real_output[:, Nd+1:], pose_label_tensor) +\\\n",
    "                                        loss_criterion(syn_output[:, :Nd+1], syn_id_label_tensor)\n",
    "                \n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                print(\"EPOCH : {0}, D : {1}\".format(epoch, d_loss.data[0]))\n",
    "                \n",
    "                # Discriminator の強さを判別\n",
    "                flag_D_strong = Is_D_strong(real_output, syn_output, id_label_tensor, pose_label_tensor, syn_id_label_tensor, Nd, condition_batchsize)\n",
    "                \n",
    "            else:\n",
    "                # Generatorの学習\n",
    "                syn_output=D(generated)\n",
    "\n",
    "                # id についての出力と元画像のラベル, poseについての出力と生成時に与えたposeコード それぞれの交差エントロピー誤差を計算\n",
    "                g_loss = loss_criterion(syn_output[:, :Nd+1], id_label_unique_tensor) +\\\n",
    "                    loss_criterion(syn_output[:, Nd+1:], pose_code_label_tensor)\n",
    "\n",
    "                optimizer_G.step()\n",
    "                print(\"EPOCH : {0}, G : {1}\".format(epoch, g_loss.data[0]))\n",
    "        \n",
    "        else:\n",
    "\n",
    "            if i%2==0:\n",
    "                # Discriminator の学習\n",
    "                real_output = D(img_tensor)\n",
    "                syn_output = D(generated.detach()) # .detach() をすることでGeneratorのパラメータを更新しない\n",
    "\n",
    "                # id についての出力とラベル, pose についての出力とラベル それぞれの交差エントロピー誤差を計算\n",
    "\n",
    "                d_loss = loss_criterion(real_output[:, :Nd+1], id_label_tensor) +\\\n",
    "                                        loss_criterion(real_output[:, Nd+1:], pose_label_tensor) +\\\n",
    "                                        loss_criterion(syn_output[:, :Nd+1], syn_id_label_tensor)\n",
    "\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                print(\"EPOCH : {0}, D : {1}\".format(epoch, d_loss.data[0]))\n",
    "                \n",
    "                # Discriminator の強さを判別\n",
    "                flag_D_strong = Is_D_strong(real_output, syn_output, id_label_tensor, pose_label_tensor, syn_id_label_tensor, Nd, minibatch_size)\n",
    "                \n",
    "            else:\n",
    "                # Generatorの学習\n",
    "                syn_output=D(generated)\n",
    "\n",
    "                # id についての出力と元画像のラベル, poseについての出力と生成時に与えたposeコード それぞれの交差エントロピー誤差を計算\n",
    "                g_loss = loss_criterion(syn_output[:, :Nd+1], id_label_unique_tensor) +\\\n",
    "                    loss_criterion(syn_output[:, Nd+1:], pose_code_label_tensor)\n",
    "\n",
    "                optimizer_G.step()\n",
    "                print(\"EPOCH : {0}, G : {1}\".format(epoch, g_loss.data[0]))\n",
    "    \n",
    "    \n",
    "    # 各エポックで学習したモデルを保存，\n",
    "    torch.save(D, \"D.model\")\n",
    "    torch.save(G, \"G.model\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
